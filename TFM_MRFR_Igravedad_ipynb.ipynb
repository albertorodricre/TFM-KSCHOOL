{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TFM Modelo predicción de índice de gravedad"
      ],
      "metadata": {
        "id": "tJNsvXJROU_u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pg6spZ6nVJR"
      },
      "outputs": [],
      "source": [
        "# Carga de datos\n",
        "import pandas as pd\n",
        "data = pd.read_excel(\"df_final_con_gravedad.xlsx\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "pIoxbGsKoFum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Procesamiento de datos**\n",
        "\n",
        "En esta etapa se llevará a cabo la preparación y depuración del conjunto de datos, con el objetivo de dejarlo en condiciones óptimas para la fase de entrenamiento del modelo. Este paso es fundamental, ya que influye de manera directa en el rendimiento y la calidad de los resultados obtenidos."
      ],
      "metadata": {
        "id": "u9mincwnO9kO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se asegura que dia_hora está en formato datetime\n",
        "data['dia_hora'] = pd.to_datetime(data['dia_hora'])\n",
        "\n",
        "# Separar en dos columnas para luego hacer la clasificacion de franja horaria\n",
        "data['fecha'] = data['dia_hora'].dt.date\n",
        "data['hora'] = data['dia_hora'].dt.time\n",
        "\n"
      ],
      "metadata": {
        "id": "yOTiOIFuoWve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se eliminan las columnas que no aporten informacion al modelo\n",
        "drop_columns = [\n",
        "    'num_expediente',\n",
        "    'dia_hora',\n",
        "    'tipo_accidente',\n",
        "    \"Peatones\",\n",
        "    \"total_implicados\",\n",
        "    \"diversidad_vehiculos\",\n",
        "    \"categoria_gravedad\",\n",
        "    \"Conductores\"\n",
        "]\n",
        "data.drop(drop_columns, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "Uwff4U0XQCrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#La columna de estado meteorológico se separa en nuevas columnas de conteo de cada precipitación\n",
        "data[\"Despejado\"]=(data[\"estado_meteorológico\"]==\"Despejado\").astype(int)\n",
        "data[\"Lluvia débil\"]=(data[\"estado_meteorológico\"]==\"Lluvia débil\").astype(int)\n",
        "data[\"Nublado\"]=(data[\"estado_meteorológico\"]==\"Nublado\").astype(int)\n",
        "data[\"Lluvia intensa\"]=(data[\"estado_meteorológico\"]==\"Lluvia intensa\").astype(int)\n",
        "data['Granizando']=(data[\"estado_meteorológico\"]=='Granizando').astype(int)\n",
        "data['Nevando']=(data[\"estado_meteorológico\"]=='Nevando').astype(int)\n",
        "\n",
        "data=data.drop(columns=[\"estado_meteorológico\"])"
      ],
      "metadata": {
        "id": "rgMlQbxQa2t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupar por 'distrito', 'fecha' y 'franja_horaria' y aplicamos funciones de agregación.\n",
        "agrupacion_completa = data.groupby(['distrito', 'fecha', 'franja_horaria']).agg(\n",
        "    total_pasajeros=('Pasajeros', 'median'),\n",
        "    vehiculo_dos_ruedas=('Vehículo de dos ruedas', lambda x: int((x != 0).any())),\n",
        "    vehiculo_pesado=('Vehículo pesado', lambda x: int((x != 0).any())),\n",
        "    turismo=('Turismo', lambda x: int((x != 0).any())),\n",
        "    otros_vehiculos=('Otros vehículos', lambda x: int((x != 0).any())),\n",
        "    tiene_vulnerables=('tiene_vulnerables', 'sum'),\n",
        "    indice_gravedad=('indice_gravedad', 'mean'),\n",
        "    Despejado=('Despejado', 'sum'),\n",
        "    Lluvia_débil=('Lluvia débil', 'sum'),\n",
        "    Lluvia_intensa=('Lluvia intensa', 'sum'),\n",
        "    Nublado=('Nublado', 'sum'),\n",
        "    Granizo= ('Granizando', 'sum'),\n",
        "    Nevando= ('Nevando', 'sum'),\n",
        ").reset_index()"
      ],
      "metadata": {
        "id": "2WE7_K-Aa7Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agrupacion_completa.head(10)"
      ],
      "metadata": {
        "id": "8otv7FIHbo82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Se compara el numero de filas esperado (nº distritos por las 4 franjas horarias y por todos los dias de año)\n",
        "# Se renombra como df para simplificar\n",
        "df = agrupacion_completa\n",
        "\n",
        "# Valores únicos\n",
        "distritos = df['distrito'].unique()\n",
        "fechas = pd.date_range(start='2021-01-01', end='2024-12-31', freq='D')\n",
        "franjas = ['Madrugada', 'Mañana', 'Tarde', 'Noche']\n",
        "\n",
        "# Total esperado\n",
        "total_esperado = len(distritos) * len(fechas) * len(franjas)\n",
        "print(f\"Registros esperados: {total_esperado}\")\n",
        "print(f\"Registros actuales: {len(df)}\")\n"
      ],
      "metadata": {
        "id": "cUuxpL41PeLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al agrupar los datos por fecha (del 1 de enero de 2021 al 31 de diciembre de 2024), distrito (21 distritos de Madrid) y franja horaria (4 intervalos definidos), el número esperado de registros en el dataset es de 122,724. Sin embargo, el resultado obtenido es de 54,179 registros.\n",
        "\n",
        "Esta diferencia se explica porque no todos los días, en cada distrito y en cada franja horaria, ocurren accidentes. En consecuencia, al realizar la agregación, aquellas combinaciones en las que no se registra ningún accidente no aparecen reflejadas en el dataset.\n",
        "\n",
        "La solución consiste en generar el producto cartesiano de todas las combinaciones posibles entre fecha, distrito y franja horaria, para posteriormente realizar un merge con el dataset original. De esta forma, las combinaciones ausentes se incorporan y las columnas correspondientes a los accidentes se completan con valores 0, garantizando así una representación consistente y completa de todas las posibles combinaciones."
      ],
      "metadata": {
        "id": "XQ885ldiAakG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "df['fecha'] = pd.to_datetime(df['fecha'])\n",
        "\n",
        "# Producto cartesiano de todas las combinaciones posibles\n",
        "combinaciones = pd.DataFrame(itertools.product(distritos, fechas, franjas),\n",
        "                             columns=['distrito', 'fecha', 'franja_horaria'])\n",
        "df_completo = pd.merge(combinaciones, df,\n",
        "                       on=['distrito', 'fecha', 'franja_horaria'],\n",
        "                       how='left')\n",
        "columnas_a_rellenar = [\n",
        "\n",
        "    'total_pasajeros',\n",
        "    'vehiculo_dos_ruedas',\n",
        "    'vehiculo_pesado',\n",
        "    'turismo', 'otros_vehiculos',\n",
        "    \"indice_gravedad\",\n",
        "    'Despejado', 'Lluvia_débil', 'Lluvia_intensa',\n",
        "    'Nublado','Granizo', 'Nevando'\n",
        "]\n",
        "\n",
        "# Se rellenan NaNs con 0 en estas columnas\n",
        "df_completo[columnas_a_rellenar] = df_completo[columnas_a_rellenar].fillna(0)\n",
        "columnas_a_ajustar = [\n",
        "\n",
        "    'total_pasajeros',\n",
        "    'vehiculo_dos_ruedas',\n",
        "    'vehiculo_pesado',\n",
        "    \"turismo\",\n",
        "    'otros_vehiculos',\n",
        "    \"tiene_vulnerables\"\n",
        "]\n",
        "\n",
        "df_completo.loc[df_completo['indice_gravedad'] == 0, columnas_a_ajustar] = 0\n",
        "\n",
        "df_completo.head()\n",
        "print(f\"Registros actuales: {len(df_completo)}\")"
      ],
      "metadata": {
        "id": "QUlNqCvVTymw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos la variable día_semana\n",
        "dias_esp = {\n",
        "    'Monday': 'Lunes',\n",
        "    'Tuesday': 'Martes',\n",
        "    'Wednesday': 'Miércoles',\n",
        "    'Thursday': 'Jueves',\n",
        "    'Friday': 'Viernes',\n",
        "    'Saturday': 'Sábado',\n",
        "    'Sunday': 'Domingo'\n",
        "}\n",
        "df_completo['dia_semana'] = df_completo['fecha'].dt.day_name().map(dias_esp)"
      ],
      "metadata": {
        "id": "FRYrQV-YdPud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se separa la fecha en mes y dia, esto ya que el modelo no aprende de formato datetime\n",
        "# Eliminamos el año ya que el modelo va a predecir con un año distinto al de aprendizaje\n",
        "# Aseguramos de nuevo que la variable fecha está en formato correcto\n",
        "df_completo[\"fecha\"] = pd.to_datetime(df_completo[\"fecha\"])\n",
        "\n",
        "# Extrae año, mes y día\n",
        "df_completo[\"mes\"] = df_completo[\"fecha\"].dt.month\n",
        "df_completo[\"día\"] = df_completo[\"fecha\"].dt.day\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4roU8pVBdj1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ahora vamos a crear una columna llamada festivo, que nos dice si es un día festivo o no\n",
        "!pip install holidays\n",
        "import holidays\n",
        "\n",
        "festivos = holidays.country_holidays('ES', subdiv='MD')\n",
        "df_completo['es_festivo'] = df_completo['fecha'].dt.date.apply(lambda x: 1 if x in festivos else 0)\n",
        "\n",
        "#ELiminamos fecha\n",
        "df_completo=df_completo.drop(columns=[\"fecha\"])"
      ],
      "metadata": {
        "id": "A4FTgYwwWQ1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_completo.info()"
      ],
      "metadata": {
        "id": "LDGiWNSLe0Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creación del modelo"
      ],
      "metadata": {
        "id": "YJfcekVfWT2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Se detectan las columnas categóricas automáticamente (dtype == object)\n",
        "categorical_cols = df_completo.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Se elimina la variable indice_gravedad de la codificación\n",
        "if 'indice_gravedad' in categorical_cols:\n",
        "    categorical_cols.remove('indice_gravedad')\n",
        "\n",
        "# Se aplica One-Hot Encoding con pandas\n",
        "data_encoded = pd.get_dummies(df_completo, columns=categorical_cols, drop_first=True)"
      ],
      "metadata": {
        "id": "fSz430x1o3Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se separan los datos en X_data e y_data\n",
        "X_data = data_encoded.drop(\"indice_gravedad\", axis=1)\n",
        "y_data = data_encoded[\"indice_gravedad\"]"
      ],
      "metadata": {
        "id": "-26pg3bipFLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=42, shuffle= True)\n"
      ],
      "metadata": {
        "id": "msTnD5iWpPe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMportaciones para el modeladp\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ],
      "metadata": {
        "id": "svHmWTSypP1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalamos xgboost si fuera necesario e importamos\n",
        "!pip install xgboost\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Convertir datos a DMatrix para la entrada de los datos al modelo xgboost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dval = xgb.DMatrix(X_val, label=y_val)\n",
        "\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 6,\n",
        "    'subsample': 0.8,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "evals = [(dval, 'eval'), (dtrain, 'train')]\n",
        "\n",
        "model_xb = xgb.train(params, dtrain, num_boost_round=100,\n",
        "                  evals=evals,\n",
        "                  early_stopping_rounds=10,\n",
        "                  verbose_eval=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "eB_pUHEm6lvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicción\n",
        "y_train_pred = model_xb.predict(dtrain)\n",
        "y_val_pred = model_xb.predict(dval)\n",
        "\n",
        "# Métricas\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
        "\n",
        "mse_val = mean_squared_error(y_val, y_val_pred)\n",
        "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
        "\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "r2_val = r2_score(y_val, y_val_pred)\n",
        "\n",
        "print(f\"Train MSE: {mse_train:.4f}, MAE: {mae_train:.4f}\")\n",
        "print(f\"Val MSE: {mse_val:.4f}, MAE: {mae_val:.4f}\")\n",
        "print(f\"Train R2: {r2_train:.4f}, Val R2: {r2_val:.4f}\")\n"
      ],
      "metadata": {
        "id": "PAcjaqRTZ-9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Obtenemos importancias desde el Booster\n",
        "scores = model_xb.get_score(importance_type='gain')  # también puedes usar 'weight' o 'cover'\n",
        "\n",
        "# Convertimos a DataFrame\n",
        "importancia_df = pd.DataFrame({\n",
        "    'Variable': list(scores.keys()),\n",
        "    'Importancia': list(scores.values())\n",
        "}).sort_values(by='Importancia', ascending=False)\n",
        "\n",
        "# Mostramos\n",
        "print(importancia_df)\n",
        "\n",
        "# Gráfico\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(importancia_df['Variable'], importancia_df['Importancia'], color='skyblue')\n",
        "plt.xlabel('Importancia')\n",
        "plt.title('Importancia de las variables en XGBoost (gain)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "snQnTl0MfAD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sistema de widgets para simulacro de funcionamiento"
      ],
      "metadata": {
        "id": "CHi92T2DaII2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se importan los datos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import unicodedata  # para normalizar y quitar tildes\n",
        "\n",
        "# Se cran listan con los los valores de los widgets\n",
        "\n",
        "distritos = [\n",
        "    'CENTRO', 'ARGANZUELA', 'RETIRO', 'SALAMANCA', 'CHAMARTÍN', 'TETUÁN',\n",
        "    'CHAMBERÍ', 'FUENCARRAL-EL PARDO', 'MONCLOA-ARAVACA', 'LATINA', 'CARABANCHEL',\n",
        "    'USERA', 'PUENTE DE VALLECAS', 'MORATALAZ', 'CIUDAD LINEAL', 'HORTALEZA',\n",
        "    'VILLAVERDE', 'VILLA DE VALLECAS', 'VICÁLVARO', 'SAN BLAS-CANILLEJAS', 'BARAJAS'\n",
        "]\n",
        "\n",
        "franjas_horarias = ['Madrugada', 'Mañana', 'Tarde', 'Noche']\n",
        "\n",
        "climas = ['Despejado', 'Lluvia_débil', 'Lluvia_intensa', 'Nublado', 'Granizo', 'Nevando']\n",
        "\n",
        "dias_semana = [\n",
        "    'Lunes', 'Martes', 'Miércoles', 'Jueves', 'Viernes', 'Sábado', 'Domingo'\n",
        "]\n",
        "\n",
        "# Se definen los widgets\n",
        "\n",
        "franja_horaria = widgets.Dropdown(options=franjas_horarias, description='Franja:')\n",
        "pasajeros = widgets.FloatSlider(min=0, max=50, step=1, description='Pasajeros:')\n",
        "vehiculo_dos_ruedas = widgets.FloatSlider(min=0, max=1, step=1, description='2 ruedas:')\n",
        "vehiculo_pesado = widgets.FloatSlider(min=0, max=1, step=1, description='Pesados:')\n",
        "turismo = widgets.FloatSlider(min=0, max=1, step=1, description='Turismos:')\n",
        "otros_vehiculos = widgets.FloatSlider(min=0, max=1, step=1, description='Otros veh.:')\n",
        "mes = widgets.IntSlider(min=1, max=12, description='Mes:')\n",
        "dia = widgets.IntSlider(min=1, max=31, description='Día:')\n",
        "es_festivo= widgets.Checkbox(value=False, description=\"Festivo\")\n",
        "dia_semana = widgets.Dropdown(options=dias_semana, description='Día de la semana:')\n",
        "tiene_vulnerables = widgets.Checkbox(value=False, description='¿Vulnerables?')\n",
        "\n",
        "\n",
        "\n",
        "clima_widgets = {clima: widgets.Checkbox(value=False, description=clima) for clima in climas}\n",
        "boton = widgets.Button(description=\"Predecir accidentes por distrito\")\n",
        "\n",
        "inputs = widgets.VBox([\n",
        "    franja_horaria,\n",
        "    pasajeros,\n",
        "    vehiculo_dos_ruedas, vehiculo_pesado, turismo, otros_vehiculos,\n",
        "    mes, dia, dia_semana, es_festivo, tiene_vulnerables,\n",
        "    widgets.HTML(value=\"<b>Condiciones climáticas:</b>\"),\n",
        "    *clima_widgets.values(), boton\n",
        "])\n",
        "\n",
        "display(inputs)\n",
        "\n",
        "# Se cargan los datos geográficos\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/codeforamerica/click_that_hood/master/public/data/madrid-districts.geojson\"\n",
        "gdf = gpd.read_file(url)\n",
        "gdf.rename(columns={\"name\": \"NOMBRE\"}, inplace=True)\n",
        "\n",
        "def normalizar_nombre(s):\n",
        "    s = s.upper()\n",
        "    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')  # quita tildes\n",
        "    s = s.replace('-', ' ')\n",
        "    s = s.strip()\n",
        "    if s == \"SAN BLAS CANILLEJAS\":\n",
        "        s = \"SAN BLAS\"\n",
        "    return s\n",
        "\n",
        "gdf[\"NOMBRE_norm\"] = gdf[\"NOMBRE\"].apply(normalizar_nombre)\n",
        "\n",
        "# Función para mostrar el mapa\n",
        "\n",
        "def mostrar_mapa(df_pred):\n",
        "    df_pred['distrito_norm'] = df_pred['distrito'].apply(normalizar_nombre)\n",
        "    df_merge = gdf.merge(df_pred, left_on='NOMBRE_norm', right_on='distrito_norm', how='left')\n",
        "\n",
        "    min_pred = df_merge[\"predicción_Índice_Gravedad\"].min()\n",
        "    df_merge['escala_color'] = df_merge[\"predicción_Índice_Gravedad\"] / (min_pred if min_pred != 0 else 1)\n",
        "\n",
        "    cmap = LinearSegmentedColormap.from_list(\n",
        "        \"azul_madrid\",\n",
        "        [\"#dbe9f6\", \"#6baed6\", \"#08519c\"]\n",
        "    )\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    df_merge.plot(column='escala_color', cmap=cmap, linewidth=0.8, edgecolor='0.8', legend=False, ax=ax)\n",
        "\n",
        "    # Añadir los valores predichos en el centro de cada polígono\n",
        "    for idx, row in df_merge.iterrows():\n",
        "        if pd.notnull(row[\"predicción_Índice_Gravedad\"]):\n",
        "            centroid = row[\"geometry\"].centroid\n",
        "            ax.text(centroid.x, centroid.y,\n",
        "                    f'{row[\"predicción_Índice_Gravedad\"]:.2f}',\n",
        "                    horizontalalignment='center',\n",
        "                    fontsize=8,\n",
        "                    color='black')\n",
        "\n",
        "    # Calcular la media y generar el mensaje\n",
        "    media = df_merge[\"predicción_Índice_Gravedad\"].mean()\n",
        "    if 2 <= media < 3.5:\n",
        "        mensaje = (\"El nivel de riesgo se considera moderado;\\n\"\n",
        "        \"se recomienda mantener medidas básicas de precaución\")\n",
        "    elif 3.5 <= media < 5.5:\n",
        "        mensaje = (\"Se detecta un nivel de riesgo relevante;\\n\"\n",
        "        \"se aconseja aumentar la atención durante la conducción.\")\n",
        "    elif media >= 5.5:\n",
        "        mensaje = (\"El nivel de riesgo es elevado;\\n\"\n",
        "        \"se recomienda extremar las precauciones y evitar zonas de alta concentración vehicular\")\n",
        "    else:\n",
        "        mensaje = (\"El nivel de riesgo es bajo;\\n\"\n",
        "        \"no se requieren medidas adicionales.\")\n",
        "\n",
        "    # Añadir el texto debajo del mapa\n",
        "    plt.figtext(0.5, 0.02,\n",
        "                f\"El índice de gravedad promedio es de {media:.2f}. {mensaje}.\",\n",
        "                ha='center', fontsize=14)\n",
        "\n",
        "    ax.set_title(\"Predicción de accidentes por distrito\", fontsize=16)\n",
        "    ax.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "feature_names = X_train.columns.tolist()  # variable global o en scope accesible\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    rows = []\n",
        "    for distrito in distritos:\n",
        "        row = {\n",
        "            'distrito': distrito,\n",
        "            'total_pasajeros': pasajeros.value,\n",
        "            'mes': mes.value,\n",
        "            'dia': dia.value,\n",
        "            'es_festivo': es_festivo.value,\n",
        "            'dia_semana': dia_semana.value,\n",
        "            'tiene_vulnerables': tiene_vulnerables.value,\n",
        "            'franja_horaria': franja_horaria.value,\n",
        "            'vehiculo_dos_ruedas': vehiculo_dos_ruedas.value,\n",
        "            'vehiculo_pesado': vehiculo_pesado.value,\n",
        "            'turismo': turismo.value,\n",
        "            'otros_vehiculos': otros_vehiculos.value,\n",
        "        }\n",
        "        for clima in clima_widgets:\n",
        "            row[clima] = int(clima_widgets[clima].value)\n",
        "        rows.append(row)\n",
        "\n",
        "    df_input = pd.DataFrame(rows)\n",
        "\n",
        "    # One-hot encoding\n",
        "    df_encoded = pd.get_dummies(df_input)\n",
        "\n",
        "    # Añadir columnas faltantes con ceros\n",
        "    for col in feature_names:\n",
        "        if col not in df_encoded.columns:\n",
        "            df_encoded[col] = 0\n",
        "\n",
        "    # Reordenar columnas para coincidir con el entrenamiento\n",
        "    df_encoded = df_encoded[feature_names]\n",
        "\n",
        "    # Crear DMatrix para la predicción\n",
        "    dmatrix_input = xgb.DMatrix(df_encoded, feature_names=feature_names)\n",
        "\n",
        "    # Predecir con Booster\n",
        "    df_input[\"predicción_Índice_Gravedad\"] = model_xb.predict(dmatrix_input)\n",
        "\n",
        "    # Mostrar resultados\n",
        "    display(df_input[['distrito', \"predicción_Índice_Gravedad\"]].sort_values(by=\"predicción_Índice_Gravedad\", ascending=False))\n",
        "    mostrar_mapa(df_input[['distrito', \"predicción_Índice_Gravedad\"]])\n",
        "\n",
        "\n",
        "# --- Conectar botón ---\n",
        "boton.on_click(on_button_clicked)\n"
      ],
      "metadata": {
        "id": "G13wO0GW7p15"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}